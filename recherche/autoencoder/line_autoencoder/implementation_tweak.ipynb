{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d593a00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from torch.optim import Adam\n",
    "import polars as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import lightgbm as lgb  \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from utils import r2_loss, train_model, plot_r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087763dc",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "198baf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "subpath = \"/home/onyxia/work/AML/data/\"\n",
    "weights = pl.read_parquet(subpath + \"weights.parquet\")\n",
    "target = pl.read_parquet(subpath + \"target.parquet\")\n",
    "responders = pl.read_parquet(subpath + \"responders.parquet\")\n",
    "features = pl.read_parquet(subpath + \"features.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9f7555",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036c121d",
   "metadata": {},
   "source": [
    "## MLP Buider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e9f8036",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mlp(widths, activation=nn.ReLU, dropout_p=0.0, use_bn=False):\n",
    "    \"\"\"\n",
    "    Creates a Multi-Layer Perceptron (MLP) block with optional Dropout and Batch Normalization.\n",
    "\n",
    "    Args:\n",
    "        widths (list): [in_dim, h1, h2, ..., out_dim]\n",
    "        activation (nn.Module): Activation function to use (default: ReLU).\n",
    "        dropout_p (float): Probability of an element to be zeroed (default: 0.0).\n",
    "        use_bn (bool): Whether to insert BatchNorm1d layers (default: False).\n",
    "    \"\"\"\n",
    "    layers = []\n",
    "    for in_f, out_f in zip(widths[:-1], widths[1:]):\n",
    "        is_output_layer = (out_f == widths[-1])\n",
    "        layers.append(nn.Linear(in_f, out_f))\n",
    "        if not is_output_layer:\n",
    "            if use_bn:\n",
    "                layers.append(nn.BatchNorm1d(out_f))\n",
    "            layers.append(activation())\n",
    "            if dropout_p > 0.0:\n",
    "                layers.append(nn.Dropout(p=dropout_p))\n",
    "    return nn.Sequential(*layers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb3a7d1",
   "metadata": {},
   "source": [
    "## Auto-Encoder supervised & unsupervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66a5471c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, widths, dropout_p=0.0, use_bn=False):\n",
    "        \"\"\"\n",
    "        Initializes the Encoder using make_mlp with specified regularization options.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Pass regularization parameters to the underlying MLP builder\n",
    "        self.net = make_mlp(widths, dropout_p=dropout_p, use_bn=use_bn)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, widths, dropout_p=0.0, use_bn=False):\n",
    "        \"\"\"\n",
    "        Initializes the Decoder using make_mlp with specified regularization options.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Pass regularization parameters to the underlying MLP builder\n",
    "        self.net = make_mlp(widths, dropout_p=dropout_p, use_bn=use_bn)\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.net(z)\n",
    "\n",
    "\n",
    "class TaskHead(nn.Module):\n",
    "    def __init__(self, widths, dropout_p=0.0, use_bn=False):\n",
    "        \"\"\"\n",
    "        Initializes the TaskHead using make_mlp with specified regularization options.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Pass regularization parameters to the underlying MLP builder\n",
    "        self.net = make_mlp(widths, dropout_p=dropout_p, use_bn=use_bn)\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.net(z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da05846",
   "metadata": {},
   "source": [
    "## Build up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f08562bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 encoder_widths,   # e.g.: [input_dim, 256, 128, latent_dim]\n",
    "                 decoder_widths,   # e.g.: [latent_dim, 128, 256, input_dim]\n",
    "                 head_widths,      # e.g.: [latent_dim, 64, output_dim]\n",
    "                 \n",
    "                 # --- NEW PARAMETERS FOR REGULARIZATION ---\n",
    "                 dropout_p=0.0,\n",
    "                 use_bn=False\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Initializes the FullModel, passing regularization parameters to all sub-modules.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Pass all necessary regularization parameters to the Encoder, Decoder, and TaskHead\n",
    "        self.encoder = Encoder(encoder_widths, dropout_p=dropout_p, use_bn=use_bn)\n",
    "        self.decoder = Decoder(decoder_widths, dropout_p=dropout_p, use_bn=use_bn)\n",
    "        # Note: BN/Dropout are often disabled or reduced on the TaskHead if the output \n",
    "        # is critical for downstream metrics like LGBM. You can adjust this call if needed.\n",
    "        self.head = TaskHead(head_widths, dropout_p=dropout_p, use_bn=use_bn)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass: X -> Z -> (X_hat, Y_hat)\n",
    "        \"\"\"\n",
    "        # 1. Encode: Map input X to latent representation Z\n",
    "        z = self.encoder(x)\n",
    "        \n",
    "        # 2. Decode: Reconstruct input X_hat from Z (Auxiliary Task)\n",
    "        x_hat = self.decoder(z)\n",
    "        \n",
    "        # 3. Predict: Map latent Z to target Y_hat (Supervised Task)\n",
    "        y_hat = self.head(z)\n",
    "        \n",
    "        return z, x_hat, y_hat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e143c7",
   "metadata": {},
   "source": [
    "# Train real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "163944d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardization complete.\n",
      "X_train_t shape after scaling: torch.Size([1219662, 56])\n",
      "Mean of X_train_t (should be ~0): tensor([ 8.1820e-09, -9.2579e-10,  3.9096e-09,  3.6093e-09,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  5.6298e-11, -8.7575e-09, -2.0017e-10,\n",
      "         3.3816e-08,  1.4741e-07,  6.0051e-09, -1.4262e-09,  2.2563e-08,\n",
      "        -2.6489e-07,  1.3570e-07,  1.3480e-08,  2.6043e-08, -1.0290e-07,\n",
      "         2.5027e-07, -7.7481e-08,  1.3806e-08,  3.3966e-09,  9.9585e-09,\n",
      "         4.3787e-10,  3.0463e-09,  1.1084e-08,  1.8516e-09, -1.4562e-08,\n",
      "        -9.4174e-09, -1.8140e-09,  3.1277e-09,  1.8453e-09,  6.1552e-09,\n",
      "         7.5564e-09, -2.5021e-11,  5.8800e-09, -2.0267e-09, -7.1874e-09,\n",
      "        -1.4975e-08,  5.4396e-08,  1.0822e-09, -1.8134e-08,  1.2411e-08,\n",
      "        -1.4913e-08, -5.8362e-09, -5.1044e-09, -5.8315e-09, -8.1945e-10,\n",
      "        -7.4814e-09,  5.6235e-09,  6.8058e-08, -7.0085e-08,  2.7561e-08,\n",
      "        -7.8442e-09])\n",
      "Std of X_train_t (should be ~1): tensor([1.0000, 1.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "n = features.height\n",
    "n_train = int(0.8 * n)\n",
    "\n",
    "X_train = features.slice(0, n_train)\n",
    "y_train = target.slice(0, n_train)\n",
    "\n",
    "X_test = features.slice(n_train)\n",
    "y_test = target.slice(n_train)\n",
    "\n",
    "\n",
    "# --- 2. CONVERSION VERS NUMPY POUR LE CALCUL STATISTIQUE ---\n",
    "X_train_np = X_train.to_numpy()\n",
    "y_train_np = y_train.to_numpy().reshape(-1, 1)\n",
    "X_test_np = X_test.to_numpy()\n",
    "y_test_np = y_test.to_numpy().reshape(-1, 1)\n",
    "\n",
    "\n",
    "# --- 3. CALCUL DES STATISTIQUES SUR X_TRAIN ET Y_TRAIN ---\n",
    "\n",
    "# Calcul de la moyenne (mean) et de l'écart-type (std) pour les features (X)\n",
    "# Keepdims=True ensures the shape is retained for broadcasting\n",
    "X_mean = np.mean(X_train_np, axis=0, keepdims=True)\n",
    "X_std = np.std(X_train_np, axis=0, keepdims=True)\n",
    "\n",
    "# Important: Avoid division by zero if a feature has zero standard deviation\n",
    "# We replace 0 std with 1 to prevent division by zero, effectively skipping standardization for constant features.\n",
    "X_std[X_std == 0] = 1 \n",
    "\n",
    "# Calcul de la moyenne et de l'écart-type pour la cible (Y)\n",
    "Y_mean = np.mean(y_train_np, axis=0, keepdims=True)\n",
    "Y_std = np.std(y_train_np, axis=0, keepdims=True)\n",
    "Y_std[Y_std == 0] = 1 \n",
    "\n",
    "\n",
    "# --- 4. APPLICATION DE LA STANDARDISATION ---\n",
    "\n",
    "# Application sur l'ensemble d'entraînement (X et Y)\n",
    "X_train_scaled = (X_train_np - X_mean) / X_std\n",
    "y_train_scaled = (y_train_np - Y_mean) / Y_std\n",
    "\n",
    "# Application sur l'ensemble de test, en utilisant les stats de TRAIN! (X et Y)\n",
    "X_test_scaled = (X_test_np - X_mean) / X_std\n",
    "y_test_scaled = (y_test_np - Y_mean) / Y_std\n",
    "\n",
    "\n",
    "# --- 5. CONVERSION VERS TENSEURS PYTORCH ---\n",
    "\n",
    "X_train_t = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_train_scaled, dtype=torch.float32)\n",
    "\n",
    "X_test_t  = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_test_t  = torch.tensor(y_test_scaled, dtype=torch.float32)\n",
    "\n",
    "print(\"Standardization complete.\")\n",
    "print(f\"X_train_t shape after scaling: {X_train_t.shape}\")\n",
    "print(f\"Mean of X_train_t (should be ~0): {X_train_t.mean(dim=0)}\")\n",
    "print(f\"Std of X_train_t (should be ~1): {X_train_t.std(dim=0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8c5ca5",
   "metadata": {},
   "source": [
    "# Test of various architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58e687ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- DÉFINITION DES DIMENSIONS FIXES ---\n",
    "INPUT_DIM = 56 \n",
    "OUTPUT_DIM = 1 \n",
    "LATENT_DIM = 16 \n",
    "\n",
    "# --- ARCHITECTURE FIXE (4 couches linéaires en tout) ---\n",
    "ENCODER_W = [INPUT_DIM, 64, 32, LATENT_DIM]\n",
    "DECODER_W = [LATENT_DIM, 32, 64, INPUT_DIM]\n",
    "HEAD_W =    [LATENT_DIM, 32, OUTPUT_DIM] \n",
    "\n",
    "ARCHITECTURES_CONFIGS = {\n",
    "    # 1. Baseline: Référence sans aucune régularisation.\n",
    "    \"A_Baseline_None\": {\n",
    "        \"encoder_widths\": ENCODER_W,\n",
    "        \"decoder_widths\": DECODER_W,\n",
    "        \"head_widths\":    HEAD_W,\n",
    "        \"dropout_p\": 0.0,\n",
    "        \"use_bn\": False\n",
    "    },\n",
    "    \n",
    "    # 2. Test Dropout: Ajoute une régularisation pour prévenir le surapprentissage.\n",
    "    \"B_Dropout_Only\": {\n",
    "        \"encoder_widths\": ENCODER_W,\n",
    "        \"decoder_widths\": DECODER_W,\n",
    "        \"head_widths\":    HEAD_W,\n",
    "        \"dropout_p\": 0.15, # Dropout modéré\n",
    "        \"use_bn\": False\n",
    "    },\n",
    "    \n",
    "    # 3. Test Batch Norm: Ajoute la BN pour stabiliser et accélérer l'entraînement.\n",
    "    \"C_BatchNorm_Only\": {\n",
    "        \"encoder_widths\": ENCODER_W,\n",
    "        \"decoder_widths\": DECODER_W,\n",
    "        \"head_widths\":    HEAD_W,\n",
    "        \"dropout_p\": 0.0,\n",
    "        \"use_bn\": True\n",
    "    },\n",
    "    \n",
    "    # 4. Test Full Reg: Combine BN et un Dropout plus élevé pour maximiser la généralisation.\n",
    "    \"D_Full_Regularization\": {\n",
    "        \"encoder_widths\": ENCODER_W,\n",
    "        \"decoder_widths\": DECODER_W,\n",
    "        \"head_widths\":    HEAD_W,\n",
    "        \"dropout_p\": 0.25, # Dropout plus important\n",
    "        \"use_bn\": True\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c55db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- STARTING ARCHITECTURAL EXPERIMENTS ---\n",
      "Total Epochs: 30 | LR: 0.001 | Alpha (Rec): 1.0\n",
      "\n",
      "=======================================================\n",
      "RUNNING EXPERIMENT: A_Baseline_None\n",
      " -> Dropout: 0.0, BatchNorm: False\n",
      " -> Encoder: [56, 64, 32, 16]\n",
      "=======================================================\n",
      "Epoch 01 | loss=3091.360 | R2_sup_NN=-0.0046 | R2_sup_LGBM=-0.0021 | R2_rec=0.3072\n",
      "Epoch 02 | loss=2792.084 | R2_sup_NN=-0.0038 | R2_sup_LGBM=-0.0072 | R2_rec=0.3774\n",
      "Epoch 03 | loss=2732.474 | R2_sup_NN=-0.0106 | R2_sup_LGBM=-0.0061 | R2_rec=0.4420\n",
      "Epoch 04 | loss=2716.511 | R2_sup_NN=-0.0168 | R2_sup_LGBM=-0.0060 | R2_rec=0.4689\n",
      "Epoch 05 | loss=2706.618 | R2_sup_NN=-0.0180 | R2_sup_LGBM=-0.0049 | R2_rec=0.4582\n",
      "Epoch 06 | loss=2700.508 | R2_sup_NN=-0.0252 | R2_sup_LGBM=-0.0073 | R2_rec=0.4608\n",
      "Epoch 07 | loss=2694.035 | R2_sup_NN=-0.0174 | R2_sup_LGBM=-0.0151 | R2_rec=0.4892\n",
      "Epoch 08 | loss=2688.460 | R2_sup_NN=-0.0312 | R2_sup_LGBM=-0.0118 | R2_rec=0.4731\n",
      "Epoch 09 | loss=2684.714 | R2_sup_NN=-0.0170 | R2_sup_LGBM=-0.0117 | R2_rec=0.5037\n",
      "Epoch 10 | loss=2681.013 | R2_sup_NN=-0.0471 | R2_sup_LGBM=-0.0158 | R2_rec=0.4641\n",
      "Epoch 11 | loss=2676.460 | R2_sup_NN=-0.0382 | R2_sup_LGBM=-0.0136 | R2_rec=0.4965\n",
      "Epoch 12 | loss=2670.062 | R2_sup_NN=-0.2296 | R2_sup_LGBM=-0.0211 | R2_rec=0.4413\n",
      "Epoch 13 | loss=2666.814 | R2_sup_NN=-0.0180 | R2_sup_LGBM=-0.0167 | R2_rec=0.4693\n",
      "Epoch 14 | loss=2662.901 | R2_sup_NN=-0.4193 | R2_sup_LGBM=-0.0251 | R2_rec=0.4529\n",
      "Epoch 15 | loss=2657.091 | R2_sup_NN=-0.2129 | R2_sup_LGBM=-0.0141 | R2_rec=0.4671\n",
      "Epoch 16 | loss=2655.006 | R2_sup_NN=-0.0803 | R2_sup_LGBM=-0.0204 | R2_rec=0.4430\n",
      "Epoch 17 | loss=2650.556 | R2_sup_NN=-0.0715 | R2_sup_LGBM=-0.0152 | R2_rec=0.4428\n",
      "Epoch 18 | loss=2647.036 | R2_sup_NN=-0.1633 | R2_sup_LGBM=-0.0191 | R2_rec=0.4454\n",
      "Epoch 19 | loss=2644.742 | R2_sup_NN=-0.0659 | R2_sup_LGBM=-0.0140 | R2_rec=0.4737\n",
      "Epoch 20 | loss=2640.764 | R2_sup_NN=-0.0453 | R2_sup_LGBM=-0.0171 | R2_rec=0.4361\n",
      "Epoch 21 | loss=2638.076 | R2_sup_NN=-0.1727 | R2_sup_LGBM=-0.0144 | R2_rec=0.4289\n",
      "Epoch 22 | loss=2633.851 | R2_sup_NN=-0.1050 | R2_sup_LGBM=-0.0155 | R2_rec=0.4349\n",
      "Epoch 23 | loss=2631.592 | R2_sup_NN=-0.0694 | R2_sup_LGBM=-0.0222 | R2_rec=0.4373\n",
      "Epoch 24 | loss=2628.283 | R2_sup_NN=-0.1083 | R2_sup_LGBM=-0.0152 | R2_rec=0.4380\n",
      "Epoch 25 | loss=2624.969 | R2_sup_NN=-0.1291 | R2_sup_LGBM=-0.0132 | R2_rec=0.4289\n",
      "Epoch 26 | loss=2621.392 | R2_sup_NN=-0.2342 | R2_sup_LGBM=-0.0115 | R2_rec=0.4080\n",
      "Epoch 27 | loss=2620.506 | R2_sup_NN=-0.0940 | R2_sup_LGBM=-0.0147 | R2_rec=0.4155\n",
      "Epoch 28 | loss=2616.217 | R2_sup_NN=-0.2121 | R2_sup_LGBM=-0.0091 | R2_rec=0.4339\n",
      "Epoch 29 | loss=2612.917 | R2_sup_NN=-0.1167 | R2_sup_LGBM=-0.0268 | R2_rec=0.3857\n",
      "Epoch 30 | loss=2612.880 | R2_sup_NN=-0.1902 | R2_sup_LGBM=-0.0123 | R2_rec=0.4200\n",
      "\n",
      "=======================================================\n",
      "RUNNING EXPERIMENT: B_Dropout_Only\n",
      " -> Dropout: 0.15, BatchNorm: False\n",
      " -> Encoder: [56, 64, 32, 16]\n",
      "=======================================================\n",
      "Epoch 01 | loss=3589.401 | R2_sup_NN=-0.0042 | R2_sup_LGBM=-0.0108 | R2_rec=0.1092\n",
      "Epoch 02 | loss=3412.958 | R2_sup_NN=-0.0019 | R2_sup_LGBM=-0.0307 | R2_rec=0.1454\n",
      "Epoch 03 | loss=3363.713 | R2_sup_NN=-0.0004 | R2_sup_LGBM=-0.0077 | R2_rec=0.1609\n",
      "Epoch 04 | loss=3329.723 | R2_sup_NN=-0.0003 | R2_sup_LGBM=-0.0051 | R2_rec=0.2039\n",
      "Epoch 05 | loss=3312.035 | R2_sup_NN=0.0008 | R2_sup_LGBM=-0.0131 | R2_rec=0.1917\n",
      "Epoch 06 | loss=3283.239 | R2_sup_NN=-0.0013 | R2_sup_LGBM=-0.0103 | R2_rec=0.2426\n",
      "Epoch 07 | loss=3268.326 | R2_sup_NN=0.0007 | R2_sup_LGBM=-0.0062 | R2_rec=0.2715\n"
     ]
    }
   ],
   "source": [
    "# --- Définition des Hyperparamètres d'Entraînement Fixes ---\n",
    "# Basés sur la correction de la stagnation (LR plus faible, alpha réduit)\n",
    "FIXED_PARAMS = {\n",
    "    \"n_epochs\": 30,\n",
    "    \"batch_size\": 512,\n",
    "    \"lr\": 1e-3,      # Learning Rate stable\n",
    "    \"alpha\": 1.0,    # Poids de Reconstruction faible (pour prioriser la supervision)\n",
    "    \"beta\": 1.0,     # Poids de Supervision fort\n",
    "    \"optimizer_cls\": Adam,\n",
    "    \"loss_rec_cls\": nn.MSELoss,\n",
    "    \"loss_sup_cls\": nn.MSELoss,\n",
    "    \"weight_decay\": 1e-5 # Ajout d'une petite régularisation L2, conseillée avec Adam\n",
    "}\n",
    "\n",
    "# Dictionnaire pour stocker tous les résultats pour l'analyse finale\n",
    "all_experiment_results = {}\n",
    "\n",
    "print(\"--- STARTING ARCHITECTURAL EXPERIMENTS ---\")\n",
    "print(f\"Total Epochs: {FIXED_PARAMS['n_epochs']} | LR: {FIXED_PARAMS['lr']} | Alpha (Rec): {FIXED_PARAMS['alpha']}\")\n",
    "\n",
    "for arch_name, config in ARCHITECTURES_CONFIGS.items():\n",
    "    print(f\"\\n=======================================================\")\n",
    "    print(f\"RUNNING EXPERIMENT: {arch_name}\")\n",
    "    print(f\" -> Dropout: {config['dropout_p']}, BatchNorm: {config['use_bn']}\")\n",
    "    print(f\" -> Encoder: {config['encoder_widths']}\")\n",
    "    print(f\"=======================================================\")\n",
    "    \n",
    "    # 1. Création du modèle avec les paramètres d'architecture et de régularisation\n",
    "    # Assurez-vous que les tenseurs de données sont définis (X_train_t, etc.)\n",
    "    try:\n",
    "        model_instance = FullModel(\n",
    "            encoder_widths=config['encoder_widths'],\n",
    "            decoder_widths=config['decoder_widths'],\n",
    "            head_widths=config['head_widths'],\n",
    "            dropout_p=config['dropout_p'],\n",
    "            use_bn=config['use_bn']\n",
    "        )\n",
    "    except NameError:\n",
    "        print(\"\\nERROR: FullModel, X_train_t, or ARCHITECTURES_CONFIGS are not defined.\")\n",
    "        # Simuler un retour pour éviter l'échec total si les données ne sont pas chargées\n",
    "        break\n",
    "\n",
    "    # 2. Lancement de l'entraînement\n",
    "    r2_rec, r2_sup, r2_lgbm, losses = train_model(\n",
    "        model=model_instance,\n",
    "        X_train_t=X_train_t, y_train_t=y_train_t,\n",
    "        X_test_t=X_test_t, y_test_t=y_test_t,\n",
    "        **FIXED_PARAMS\n",
    "    )\n",
    "    \n",
    "    # 3. Stockage des résultats\n",
    "    all_experiment_results[arch_name] = {\n",
    "        'r2_rec': r2_rec, \n",
    "        'r2_sup': r2_sup, \n",
    "        'r2_lgbm': r2_lgbm, \n",
    "        'final_lgbm_r2': r2_lgbm[-1] if r2_lgbm else None,\n",
    "        'final_sup_r2': r2_sup[-1] if r2_sup else None\n",
    "    }\n",
    "    \n",
    "print(\"\\n--- ALL EXPERIMENTS COMPLETED ---\")\n",
    "\n",
    "# --- ANALYSE FINALE DES PERFORMANCES ---\n",
    "\n",
    "print(\"\\nFINAL R² LGBM PERFORMANCE COMPARISON:\")\n",
    "final_lgbm_results = {name: res['final_lgbm_r2'] for name, res in all_experiment_results.items()}\n",
    "\n",
    "# Trier et afficher les résultats pour déterminer la meilleure architecture\n",
    "sorted_results = sorted(final_lgbm_results.items(), key=lambda item: item[1] if item[1] is not None else -float('inf'), reverse=True)\n",
    "\n",
    "for name, r2 in sorted_results:\n",
    "    print(f\"- {name:<20}: R² LGBM = {r2:.4f}\")\n",
    "\n",
    "# Vous pouvez ensuite utiliser matplotlib pour visualiser l'évolution\n",
    "# plot_comparison(all_experiment_results) # Vous devrez adapter plot_comparison pour qu'elle itère sur ce dictionnaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fd486d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AML_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
